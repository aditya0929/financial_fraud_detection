{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3eYCJBFlP5r",
        "outputId": "362fd351-2588-4ec9-cdae-6ab9528c6dd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.30.0-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: importlib-metadata<8,>=1.4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (7.0.1)\n",
            "Requirement already satisfied: packaging<24,>=16.8 in /usr/local/lib/python3.10/dist-packages (from streamlit) (23.2)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.5.3)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=6.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (10.0.1)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.0)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.2.3)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.5.0)\n",
            "Requirement already satisfied: tzlocal<6,>=1.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.2)\n",
            "Collecting validators<1,>=0.2 (from streamlit)\n",
            "  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.41-py3-none-any.whl (196 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.2)\n",
            "Collecting watchdog>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.3)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<8,>=1.4->streamlit) (3.17.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3,>=2.7.3->streamlit) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2023.11.17)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.32.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.16.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Installing collected packages: watchdog, validators, smmap, scikit-learn, pydeck, gitdb, gitpython, streamlit\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.41 pydeck-0.8.1b0 scikit-learn-1.3.2 smmap-5.0.1 streamlit-1.30.0 validators-0.22.0 watchdog-3.0.0\n"
          ]
        }
      ],
      "source": [
        "# Install necessary packages\n",
        "!pip install -U scikit-learn joblib streamlit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "npPlJRMnpDxd"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "import joblib\n",
        "from google.colab import files\n",
        "import streamlit as st\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "Yl9_ILtspJ6p",
        "outputId": "f3b75267-a922-42c1-d208-a70fb1391629"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7eccc513-e844-4044-86ce-77b7fb04d2c9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7eccc513-e844-4044-86ce-77b7fb04d2c9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving credit-debit dataset.csv to credit-debit dataset.csv\n",
            "Saving TransactionDataset1.csv to TransactionDataset1.csv\n"
          ]
        }
      ],
      "source": [
        "# Function to preprocess the data\n",
        "def preprocess_data(data):\n",
        "    # Select relevant features\n",
        "    selected_features = ['age', 'transaction_amount',\n",
        "                         'average_expenditure', 'comparison_with_avg_expenditure',\n",
        "                         'transaction_count_7_days',\n",
        "                         'Total Credit Amount','fraud_indicator']\n",
        "\n",
        "    # Keep only the selected features\n",
        "    data = data[selected_features]\n",
        "\n",
        "    # Handle missing values\n",
        "    data = data.dropna()\n",
        "\n",
        "    # Label encoding for categorical variables\n",
        "    label_encoder = LabelEncoder()\n",
        "    data[data.select_dtypes(include=['object']).columns] = data.select_dtypes(include=['object']).apply(lambda col: label_encoder.fit_transform(col.astype(str)))\n",
        "\n",
        "    return data, label_encoder\n",
        "\n",
        "# Upload the dataset files\n",
        "uploaded_files = files.upload()\n",
        "\n",
        "# Load datasets\n",
        "data1_path = '/content/TransactionDataset1.csv'\n",
        "data2_path = '/content/credit-debit dataset.csv'\n",
        "\n",
        "data1 = pd.read_csv(data1_path)\n",
        "data2 = pd.read_csv(data2_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ydSYvndlpQfQ"
      },
      "outputs": [],
      "source": [
        "# Preprocess data\n",
        "preprocessed_data, label_encoder = preprocess_data(pd.concat([data1, data2], axis=1))\n",
        "\n",
        "# Separate features and target variable\n",
        "X = preprocessed_data.drop('fraud_indicator', axis=1)\n",
        "y = preprocessed_data['fraud_indicator']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Select relevant features for training\n",
        "selected_features_for_training = ['age', 'transaction_amount',\n",
        "                                   'average_expenditure', 'comparison_with_avg_expenditure',\n",
        "                                   'transaction_count_7_days',\n",
        "                                   'Total Credit Amount', ]\n",
        "\n",
        "# Use only the selected features for training\n",
        "X_train = X_train[selected_features_for_training]\n",
        "X_test = X_test[selected_features_for_training]\n",
        "\n",
        "# Use SimpleImputer to handle missing values by filling NaNs with the mean\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
        "X_test_scaled = scaler.transform(X_test_imputed)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQbF9lVFq9p_",
        "outputId": "5e3b5ee1-0046-44aa-9d05-b505a3a695d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    2.4s\n",
            "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:    8.5s\n",
            "[Parallel(n_jobs=1)]: Done 449 tasks      | elapsed:   19.3s\n",
            "[Parallel(n_jobs=1)]: Done 799 tasks      | elapsed:   34.0s\n",
            "[Parallel(n_jobs=1)]: Done 1249 tasks      | elapsed:   53.8s\n",
            "[Parallel(n_jobs=1)]: Done 1799 tasks      | elapsed:  1.3min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model, scaler, and label encoder saved as /content/random_forest_.pkl\n",
            "Features used for training:\n",
            "age\n",
            "transaction_amount\n",
            "average_expenditure\n",
            "comparison_with_avg_expenditure\n",
            "transaction_count_7_days\n",
            "Total Credit Amount\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=1)]: Done 449 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=1)]: Done 799 tasks      | elapsed:    0.7s\n",
            "[Parallel(n_jobs=1)]: Done 1249 tasks      | elapsed:    1.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.99      0.94      2562\n",
            "           1       0.97      0.78      0.86      1438\n",
            "\n",
            "    accuracy                           0.91      4000\n",
            "   macro avg       0.93      0.88      0.90      4000\n",
            "weighted avg       0.92      0.91      0.91      4000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done 1799 tasks      | elapsed:    1.5s\n"
          ]
        }
      ],
      "source": [
        "# Choose a model (Random Forest)\n",
        "model = RandomForestClassifier(n_estimators=2000, random_state=42, verbose=1)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Save the model, scaler, and label encoder\n",
        "model_filename = '/content/random_forest_.pkl'\n",
        "joblib.dump({\n",
        "    'label_encoder': label_encoder,\n",
        "    'scaler': scaler,\n",
        "    'model': model,\n",
        "    'features': selected_features_for_training  # Save the features used for training\n",
        "}, model_filename)\n",
        "\n",
        "print(f'Model, scaler, and label encoder saved as {model_filename}')\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Display the features used for training\n",
        "print(\"Features used for training:\")\n",
        "for feature_name in selected_features_for_training:\n",
        "    print(feature_name)\n",
        "\n",
        "# Make predictions on the test set\n",
        "X_test_selected = X_test[selected_features_for_training]\n",
        "X_test_selected_imputed = imputer.transform(X_test_selected)\n",
        "X_test_selected_scaled = scaler.transform(X_test_selected_imputed)\n",
        "\n",
        "y_pred = model.predict(X_test_selected_scaled)\n",
        "\n",
        "# Display classification report\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_rep)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxG7qv5Jkep5",
        "outputId": "160136d3-8f8a-43dc-86cb-edccb913bfec"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/232.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "FKAhda02rDvz"
      },
      "outputs": [],
      "source": [
        "# Save the Streamlit app code to a file\n",
        "import subprocess\n",
        "app_code = \"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import streamlit as st\n",
        "import joblib\n",
        "import sqlite3\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tempfile import NamedTemporaryFile\n",
        "import os\n",
        "import PyPDF2\n",
        "import re\n",
        "\n",
        "# Set up SQLite database connection\n",
        "conn = sqlite3.connect('/content/user_data.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Create a table for user data if it doesn't exist\n",
        "cursor.execute('''\n",
        "    CREATE TABLE IF NOT EXISTS user_data (\n",
        "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "        age INTEGER,\n",
        "        transaction_amount INTEGER,\n",
        "        average_expenditure INTEGER,\n",
        "        comparison_with_avg_expenditure INTEGER,\n",
        "        transaction_count_7_days INTEGER,\n",
        "        \"Total Credit Amount\" INTEGER,\n",
        "        prediction INTEGER,\n",
        "        timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
        "    )\n",
        "''')\n",
        "conn.commit()\n",
        "\n",
        "# Load the pre-trained model, scaler, and label encoder\n",
        "model_data = joblib.load('/content/random_forest_.pkl')\n",
        "label_encoder = model_data['label_encoder']\n",
        "scaler = model_data['scaler']\n",
        "model = model_data['model']\n",
        "features = model_data['features']\n",
        "\n",
        "# Function to extract data from PDF\n",
        "def extract_data_from_pdf(pdf_filename):\n",
        "    extracted_data = {}\n",
        "\n",
        "    with open(pdf_filename, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            text = page.extract_text()\n",
        "\n",
        "            # Define regular expressions for each attribute\n",
        "            attribute_patterns = {\n",
        "                \"age\": r\"age:\\s*(\\d+)\",\n",
        "                \"transaction_amount\": r\"transaction_amount:\\s*([\\d.]+)\",\n",
        "                \"average_expenditure\": r\"average_expenditure:\\s*([\\d.]+)\",\n",
        "                \"comparison_with_avg_expenditure\": r\"comparison_with_avg_expenditure:\\s*([\\d.]+)\",\n",
        "                \"transaction_count_7_days\": r\"transaction_count_7_days:\\s*(\\d+)\",\n",
        "                \"Total Credit Amount\": r\"Total Credit Amount:\\s*([\\d.]+)\",\n",
        "            }\n",
        "\n",
        "            # Extract values using regular expressions\n",
        "            for feature, pattern in attribute_patterns.items():\n",
        "                match = re.search(pattern, text)\n",
        "                if match:\n",
        "                    extracted_data[feature] = match.group(1)\n",
        "\n",
        "    return extracted_data\n",
        "\n",
        "# Streamlit app code\n",
        "st.title('Fraud Detection App')\n",
        "\n",
        "# File upload section\n",
        "uploaded_file = st.file_uploader(\"Upload a PDF file\", type=[\"pdf\"])\n",
        "\n",
        "# Display user input or uploaded file data\n",
        "if uploaded_file is not None:\n",
        "    # Extract data from the uploaded PDF\n",
        "    temp_pdf = NamedTemporaryFile(delete=False)\n",
        "    temp_pdf.write(uploaded_file.read())\n",
        "    temp_pdf.close()\n",
        "\n",
        "    extracted_data = extract_data_from_pdf(temp_pdf.name)\n",
        "\n",
        "    # Display the extracted data\n",
        "    st.subheader('Uploaded PDF Data:')\n",
        "    uploaded_data = pd.DataFrame(extracted_data, index=[0])\n",
        "    st.table(uploaded_data)\n",
        "\n",
        "    # Use the extracted data to fill the input space\n",
        "    user_input = {}\n",
        "    for feature in features:\n",
        "        if feature != 'fraud_indicator' and feature in extracted_data:\n",
        "            user_input[feature] = st.number_input(f'Enter {feature}', step=1, value=int(float(extracted_data[feature])))\n",
        "        else:\n",
        "            user_input[feature] = st.number_input(f'Enter {feature}', step=1, value=0)\n",
        "\n",
        "    # Display user input\n",
        "    st.subheader('User Input:')\n",
        "    user_data_input = pd.DataFrame(user_input, index=[0])\n",
        "    st.table(user_data_input)\n",
        "\n",
        "# Display user input space if no PDF is uploaded\n",
        "else:\n",
        "    st.subheader('Enter Transaction Details:')\n",
        "    user_input = {}\n",
        "\n",
        "    for feature in features:\n",
        "        if feature != 'fraud_indicator':\n",
        "            user_input[feature] = st.number_input(f'Enter {feature}', step=1, value=0)\n",
        "\n",
        "    # Display user input\n",
        "    st.subheader('User Input:')\n",
        "    user_data_input = pd.DataFrame(user_input, index=[0])\n",
        "    st.table(user_data_input)\n",
        "\n",
        "# Make prediction\n",
        "user_data_processed = user_data_input.copy()  # No need to preprocess for SQLite\n",
        "user_data_scaled = scaler.transform(user_data_processed)\n",
        "prediction = model.predict(user_data_scaled)\n",
        "\n",
        "# Insert user input and prediction into the database\n",
        "if st.button('Submit'):\n",
        "    cursor.execute('''\n",
        "        INSERT INTO user_data (\n",
        "            age,\n",
        "            transaction_amount,\n",
        "            average_expenditure,\n",
        "            comparison_with_avg_expenditure,\n",
        "            transaction_count_7_days,\n",
        "            \"Total Credit Amount\",\n",
        "            prediction\n",
        "        )\n",
        "        VALUES (?, ?, ?, ?, ?, ?, ?)\n",
        "    ''', (\n",
        "        user_input['age'],\n",
        "        user_input['transaction_amount'],\n",
        "        user_input['average_expenditure'],\n",
        "        user_input['comparison_with_avg_expenditure'],\n",
        "        user_input['transaction_count_7_days'],\n",
        "        user_input['Total Credit Amount'],\n",
        "        prediction[0]\n",
        "    ))\n",
        "    conn.commit()\n",
        "    st.success('Data submitted successfully!')\n",
        "\n",
        "    # Display prediction for fraud detection\n",
        "    st.subheader('Prediction for Fraud Detection:')\n",
        "    if prediction[0] == 1:\n",
        "        st.warning('This transaction is flagged as potentially fraudulent!')\n",
        "    else:\n",
        "        st.success('This transaction is not flagged as fraudulent!')\n",
        "\n",
        "# Streamlit app code for visualization on the sidebar\n",
        "st.sidebar.title('Fraud Detection App - Visualization')\n",
        "\n",
        "# Display user IDs on the sidebar\n",
        "user_ids = pd.read_sql_query('SELECT id FROM user_data ORDER BY timestamp DESC LIMIT 5', conn)['id'].tolist()\n",
        "selected_user_id = st.sidebar.selectbox('Select User ID:', user_ids)\n",
        "\n",
        "# Retrieve data for the selected user ID\n",
        "selected_data = pd.read_sql_query(f'SELECT * FROM user_data WHERE id={selected_user_id}', conn)\n",
        "\n",
        "# Display selected user data on the sidebar\n",
        "st.sidebar.subheader(f'Selected User Data (User ID: {selected_user_id}):')\n",
        "st.sidebar.table(selected_data)\n",
        "\n",
        "# Visualization - Transaction Amount Excess Chart\n",
        "if selected_data.shape[0] > 0:\n",
        "    st.sidebar.subheader('Transaction Amount Excess Chart')\n",
        "\n",
        "    # Threshold value for transaction amount\n",
        "    transaction_amount_threshold = 110000\n",
        "\n",
        "    # Excess transactions\n",
        "    excess_transactions = max(0, selected_data['transaction_amount'].values[0] - transaction_amount_threshold)\n",
        "\n",
        "    # Visualization\n",
        "    fig, ax = plt.subplots(figsize=(8, 4))\n",
        "    ax.barh(['Excess Transactions', 'Remaining Transactions'], [excess_transactions, transaction_amount_threshold - excess_transactions], color=['red', 'green'])\n",
        "    ax.set_xlabel('Number of Transactions')\n",
        "    ax.set_title('Transaction Amount Excess Chart')\n",
        "    st.sidebar.pyplot(fig)\n",
        "    st.sidebar.write(f'Transaction Amount Threshold: {transaction_amount_threshold}')\n",
        "\n",
        "# Visualization - Comparison with Avg Expenditure Excess Chart\n",
        "if selected_data.shape[0] > 0:\n",
        "    st.sidebar.subheader('Comparison with Avg Expenditure Excess Chart')\n",
        "\n",
        "    # Threshold value for comparison with avg expenditure\n",
        "    comparison_with_avg_expenditure_threshold = 30000\n",
        "\n",
        "    # Excess value\n",
        "    excess_value = max(0, selected_data['comparison_with_avg_expenditure'].values[0] - comparison_with_avg_expenditure_threshold)\n",
        "\n",
        "    # Visualization\n",
        "    fig, ax = plt.subplots(figsize=(8, 4))\n",
        "    ax.barh(['Excess Value', 'Remaining Value'], [excess_value, comparison_with_avg_expenditure_threshold - excess_value], color=['red', 'green'])\n",
        "    ax.set_xlabel('Value')\n",
        "    ax.set_title('Comparison with Avg Expenditure Excess Chart')\n",
        "    st.sidebar.pyplot(fig)\n",
        "    st.sidebar.write(f'Comparison with Avg Expenditure Threshold: {comparison_with_avg_expenditure_threshold}')\n",
        "\n",
        "# Visualization - Total Credit Amount Excess Chart\n",
        "if selected_data.shape[0] > 0:\n",
        "    st.sidebar.subheader('Total Credit Amount Excess Chart')\n",
        "\n",
        "    # Threshold value for total credit amount\n",
        "    total_credit_amount_threshold = 150000\n",
        "\n",
        "    # Excess value\n",
        "    excess_value_credit = max(0, selected_data['Total Credit Amount'].values[0] - total_credit_amount_threshold)\n",
        "\n",
        "    # Visualization\n",
        "    fig, ax = plt.subplots(figsize=(8, 4))\n",
        "    ax.barh(['Excess Value', 'Remaining Value'], [excess_value_credit, total_credit_amount_threshold - excess_value_credit], color=['red', 'green'])\n",
        "    ax.set_xlabel('Value')\n",
        "    ax.set_title('Total Credit Amount Excess Chart')\n",
        "    st.sidebar.pyplot(fig)\n",
        "    st.sidebar.write(f'Total Credit Amount Threshold: {total_credit_amount_threshold}')\n",
        "\n",
        "# Close the database connection when done\n",
        "conn.close()\n",
        "\n",
        "# Sidebar\n",
        "st.sidebar.title('Additional Information')\n",
        "st.sidebar.markdown('This Streamlit app is for demonstration purposes only.')\n",
        "\"\"\"\n",
        "\n",
        "with open('/content/streamlit_app.py', 'w') as f:\n",
        "    f.write(app_code)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download ngrok binary for Linux\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "\n",
        "# Unzip the downloaded file\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "\n",
        "# Move ngrok to /usr/local/bin (or any directory in your PATH)\n",
        "!sudo mv ngrok /usr/local/bin/\n",
        "\n",
        "# Clean up the downloaded files (optional)\n",
        "!rm ngrok-stable-linux-amd64.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJkCSXIsjydM",
        "outputId": "35ad95d0-8a46-455f-8ccf-39e7d0f44eeb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-17 22:33:56--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 18.205.222.128, 54.237.133.81, 52.202.168.65, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|18.205.222.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13921656 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.28M  22.2MB/s    in 0.6s    \n",
            "\n",
            "2024-01-17 22:33:57 (22.2 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13921656/13921656]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 2b2dbgzgUwpEFc6LV0p39tU1zJR_3sMRYo8mQp3KCYbxNYMh\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woISyLElj-nG",
        "outputId": "934b45a3-5383-4d93-ce1c-e8c645fd34b3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGvC4Q0nckVW",
        "outputId": "c20e7cef-473f-4712-f370-ab5b0304ca98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: ngrok: command not found\n"
          ]
        }
      ],
      "source": [
        "!ngrok config add-authtoken 2b2dbgzgUwpEFc6LV0p39tU1zJR_3sMRYo8mQp3KCYbxNYMh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrkeMFut5_2S"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "from multiprocessing import Process\n",
        "\n",
        "# Function to run a command in a separate process and capture output\n",
        "def run_command(command):\n",
        "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    stdout, stderr = process.communicate()\n",
        "    output = stdout.decode('utf-8') + stderr.decode('utf-8')\n",
        "    print(f\"Command: {command}\\nOutput: {output}\")\n",
        "    return output\n",
        "\n",
        "# Define the commands\n",
        "streamlit_command = ['streamlit', 'run', '/content/streamlit_app.py']\n",
        "localtunnel_command = ['npx', 'localtunnel', '--port', '8501']\n",
        "ngrok_command = ['ngrok', 'http', '8501']\n",
        "\n",
        "# Run each command in a separate process\n",
        "processes = [\n",
        "    Process(target=run_command, args=(streamlit_command,)),\n",
        "    Process(target=run_command, args=(localtunnel_command,)),\n",
        "    Process(target=run_command, args=(ngrok_command,))\n",
        "]\n",
        "\n",
        "# Start all processes\n",
        "for process in processes:\n",
        "    process.start()\n",
        "\n",
        "# Wait for all processes to finish\n",
        "for process in processes:\n",
        "    process.join()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_VthKOoO2uw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}